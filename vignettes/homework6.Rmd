---
title: "Homework 6"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework6}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE,
  fig.align = "center",
  fig.width = 5,
  fig.height = 5,
  rmarkdown.html_vignette.check_title = FALSE
)
```

# Question 1

## Part (a) 

Here I compute, approximately, $E(X^2)$ of $X \sim \text{Gamma}(4, 2)$ using naive Monte Carlo with 1000 samples. I find that the naive Monte Carlo estimate of $E(X^2)$ is $\boxed{75.117}$.

```{r "naive MC", echo=FALSE}
rm(list=ls())
set.seed(123)
gamma_samples_1 <- rgamma(1000, shape=4, scale=2)
second_moment_1 <- mean(gamma_samples_1^2)
second_moment_1
```


## Part (b)

Here I use importance sampling with the $\text{LogNormal}(\mu = 8, \sigma^2 =1)$ instrument/nominal distribution with 1000 samples. The ratio function $\frac{f(x)}{g(x)}$ is given by:

\begin{aligned}
\frac{f(x)}{g(x)} &= \frac{x^{k - 1}\text{exp}(-x/\theta)}{x^{-1}\text{exp}(-(\text{log}(x)  - \mu)^2/2\sigma^2)} \nonumber \\
&= x^{k} \text{exp}(-x/\theta + (\text{log}(x)  - \mu)^2/2\sigma^2) \nonumber 
\end{aligned}

Where for $\text{Gamma}(4, 2)$, $k = 4$ and $\theta = 2$ are the shape and rate parameters, respectively.

```{r "importance sampling MC",echo=FALSE}
set.seed(1211)
k = 4
theta = 2
mu = log(k*theta) - theta/4
sigma = theta/2
n = 1000
instrument_samples <- rlnorm(n, mu, sigma)
ratio_function <- function (x, k, theta, mu, sigma){
  result <- x^k * exp(-x/theta + (log(x) - mu)^2/2*sigma^2)
  return(result)
}
ratio <- ratio_function(instrument_samples, k, theta, mu, sigma)
mean_2 <- sum(instrument_samples*ratio)/sum(ratio)
second_moment_2 <- sum((instrument_samples^2)*ratio)/sum(ratio)
second_moment_2
fourth_moment_2 <- sum((instrument_samples^4)*ratio/sum(ratio))
```

I find that the importance sampling Monte Carlo estimate of $E(X^2)$ is $\boxed{75.9698}$.

# Part (c)

Here I obtain the Monte Carlo errors and $95\%$ confidence intervals of both approximations and compare them.

```{r "Monte Carlo Error and CIs", echo=FALSE}
library(kableExtra)
variance_1 <- var(gamma_samples_1^2)
variance_2 <- fourth_moment_2 - second_moment_2^2
MCerror <- function(x, n){
  MCerror <- round(sqrt(x/n), 4)
  return(MCerror)
}

CIs <- function(estimate, MCerror){
 LB <- round(estimate - 1.96*MCerror, 4)
 UB <- round(estimate + 1.96*MCerror, 4)
 return(list(LB, UB))
 }

MCerrors <- sapply(c(variance_1, variance_2), MCerror, n)

CI_1 <- CIs(second_moment_1, MCerrors[1])
CI_2 <- CIs(second_moment_2, MCerrors[2])
row1 <- cbind(MCerrors[1], t(as.vector(CI_1)))
row2 <- cbind(MCerrors[2], t(as.vector(CI_2)))
results <- rbind(row1, row2)
colnames(results) <- c("Monte Carlo Error", "2.5% quantile", "97.5% quantile")
rownames(results) <- c("Naive Monte Carlo", "Importance Sampling")
```

```{r "tab1", echo=FALSE}
results %>%
  kable(format = "html",
        align = "c",
        digits = 4,
        caption = "MC Error and Confidence Intervals",
        booktabs = TRUE) %>%
  kable_styling()
```

# Question 2

Here I consider a Markov chain, on a countable state-space $E$, that given its current state $i$ proceeds by randomly drawing another state $j$ with proposal probability $q_{ij}$ and then accepting/rejecting this proposed state with probability
\[
a_{ij} = \frac{\pi_{j}q_{ji}}{\pi_{j}q_{ji} + \pi_{i}q_{ij}}
\]

where $\bf{\pi}$ = $(\pi_{1}, \pi_{2}, \dots)$ is a probability mass function on $E$.

## Part (a)

Here I determine what the transition probabilities are.

\[
\begin{aligned}
p_{ij} &= Pr(X_{n+1} = j\,|\,X_{n}=i) \nonumber \\
&=Pr(X_{1}=j\,|\,X_{0} = i) \nonumber \\
&=a_{ij}q_{ij} \nonumber \\
&=\bigg(\frac{\pi_{j}q_{ji}}{\pi_{j}q_{ji} + \pi_{i}q_{ij}}\bigg)q_{ij}
\end{aligned}
\]

## Part (b)

Here I show that $\bf{\pi}$ is a stationary distribution of Barker's Markov chain. It suffices to show that with
\[
p_{ij} = \bigg(\frac{\pi_{j}q_{ji}}{\pi_{j}q_{ji} + \pi_{i}q_{ij}}\bigg)q_{ij}
\]
the Markov Chain satisfies the Detailed Balance Condition (DBC), i.e., 
$\pi_{i}p_{ij}= \pi_{j}p_{ji}$

\[
\begin{aligned}
\pi_{i}p_{ij} &=\pi_{i} \bigg(\frac{\pi_{j}q_{ji}}{\pi_{j}q_{ji} + \pi_{i}q_{ij}}\bigg)q_{ij} \nonumber \\
      &=\pi_{j} \bigg(\frac{\pi_{i}q_{ij}}{\pi_{j}q_{ji} + \pi_{i}q_{ij}}\bigg)q_{ji} \nonumber \\
      &= \pi_{j}p_{ji}
\end{aligned}
\]


# Question 3























































































































