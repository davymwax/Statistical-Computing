---
title: "Homework 6"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework6}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE,
  fig.align = "center",
  fig.width = 5,
  fig.height = 5,
  rmarkdown.html_vignette.check_title = FALSE
)
library(kableExtra)
```


# Question 1

## Part (a) 

Here I compute, approximately, $E(X^2)$ of $X \sim \text{Gamma}(4, 2)$ using naive Monte Carlo with 1000 samples. I find the following naive Monte Carlo estimate of $E(X^2)$.

```{r "naive MC", echo=FALSE}
rm(list=ls())
set.seed(123)
gamma_samples_1 <- rgamma(1000, shape=4, scale=2)
second_moment_1 <- mean(gamma_samples_1^2)
second_moment_1 %>%
  kable(format = "html",
        digits = 4,
        caption = "Naive Sampling Estimate of Second Moment",
        align = "c",
        booktabs = TRUE)
```


## Part (b)

Here I use importance sampling with the $\text{LogNormal}(\mu = 8, \sigma^2 =1)$ instrument/nominal distribution with 1000 samples. The ratio function $\frac{f(x)}{g(x)}$ is given by:

\begin{aligned}
\frac{f(x)}{g(x)} &=C \frac{x^{k - 1}\text{exp}(-x/\theta)}{x^{-1}\text{exp}(-(\text{log}(x)  - \mu)^2/2\sigma^2)} \nonumber \\
&= C x^{k} \text{exp}(-x/\theta + (\text{log}(x)  - \mu)^2/2\sigma^2) \nonumber 
\end{aligned}

Where for $\text{Gamma}(4, 2)$, $k = 4$ and $\theta = 2$ are the shape and rate parameters, respectively. I find that the following importance sampling Monte Carlo estimate of $E(X^2)$.

```{r "importance sampling MC",echo=FALSE}
set.seed(1211)
k = 4
theta = 2
mu = log(k*theta) - theta/4
sigma = theta/2
n = 1000
instrument_samples <- rlnorm(n, mu, sigma)
ratio_function <- function (x, k, theta, mu, sigma){
  result <- x^k * exp(-x/theta + (log(x) - mu)^2/2*sigma^2)
  return(result)
}
ratio <- ratio_function(instrument_samples, k, theta, mu, sigma)
mean_2 <- sum(instrument_samples*ratio)/sum(ratio)
second_moment_2 <- sum((instrument_samples^2)*ratio)/sum(ratio)
fourth_moment_2 <- sum((instrument_samples^4)*ratio/sum(ratio))
second_moment_2 %>%
  kable(format = "html",
        digits = 4,
        align = "c",
        caption="Importance Sampling Estimate \n of the Second Moment",
        booktabs=TRUE)
```

# Part (c)

Here I obtain the Monte Carlo errors and $95\%$ confidence intervals of both approximations and compare them.

```{r "Monte Carlo Error and CIs", echo=FALSE}
variance_1 <- var(gamma_samples_1^2)
variance_2 <- fourth_moment_2 - second_moment_2^2
MCerror <- function(x, n){
  MCerror <- round(sqrt(x/n), 4)
  return(MCerror)
}

CIs <- function(estimate, MCerror){
 LB <- round(estimate - 1.96*MCerror, 4)
 UB <- round(estimate + 1.96*MCerror, 4)
 return(list(LB, UB))
 }

MCerrors <- sapply(c(variance_1, variance_2), MCerror, n)

CI_1 <- CIs(second_moment_1, MCerrors[1])
CI_2 <- CIs(second_moment_2, MCerrors[2])
row1 <- cbind(MCerrors[1], t(as.vector(CI_1)))
row2 <- cbind(MCerrors[2], t(as.vector(CI_2)))
results <- rbind(row1, row2)
colnames(results) <- c("Monte Carlo Error", "2.5% quantile", "97.5% quantile")
rownames(results) <- c("Naive Monte Carlo", "Importance Sampling")
```

```{r "tab1", echo=FALSE}
results %>%
  kable(format = "html",
        align = "c",
        digits = 4,
        caption = "MC Error and Confidence Intervals",
        booktabs = TRUE) %>%
  kable_styling()
```

# Question 2

Here I consider a Markov chain, on a countable state-space $E$, that given its current state $i$ proceeds by randomly drawing another state $j$ with proposal probability $q_{ij}$ and then accepting/rejecting this proposed state with probability
\[
a_{ij} = \frac{\pi_{j}q_{ji}}{\pi_{j}q_{ji} + \pi_{i}q_{ij}}
\]

where $\bf{\pi}$ = $(\pi_{1}, \pi_{2}, \dots)$ is a probability mass function on $E$.

## Part (a)

Here I determine what the transition probabilities are.

\[
\begin{aligned}
p_{ij} &= Pr(X_{n+1} = j\,|\,X_{n}=i) \nonumber \\
&=Pr(X_{1}=j\,|\,X_{0} = i) \nonumber \\
&=a_{ij}q_{ij} \nonumber \\
&=\bigg(\frac{\pi_{j}q_{ji}}{\pi_{j}q_{ji} + \pi_{i}q_{ij}}\bigg)q_{ij}
\end{aligned}
\]

## Part (b)

Here I show that $\bf{\pi}$ is a stationary distribution of Barker's Markov chain. It suffices to show that with
\[
p_{ij} = \bigg(\frac{\pi_{j}q_{ji}}{\pi_{j}q_{ji} + \pi_{i}q_{ij}}\bigg)q_{ij}
\]
the Markov Chain satisfies the Detailed Balance Condition (DBC), i.e., 
$\pi_{i}p_{ij}= \pi_{j}p_{ji}$

\[
\begin{aligned}
\pi_{i}p_{ij} &=\pi_{i} \bigg(\frac{\pi_{j}q_{ji}}{\pi_{j}q_{ji} + \pi_{i}q_{ij}}\bigg)q_{ij} \nonumber \\
      &=\pi_{j} \bigg(\frac{\pi_{i}q_{ij}}{\pi_{j}q_{ji} + \pi_{i}q_{ij}}\bigg)q_{ji} \nonumber \\
      &= \pi_{j}p_{ji}
\end{aligned}
\]


# Question 3

We want to approximate $E(X^2)$ from $\text{Gamma}(4, 2)$. $X \sim \text{Gamma}(4,2)$ looks like a right-skewed normal. So $\text{log}(X) \sim N(\mu, \sigma^2)$. So my idea for the Rosenbluth-Hastings algorithm is to simulate $Y = \text{log}(X)$ from a $N(0, \sigma^2)$, where $\sigma^2$ can be tuned appropriately because we know how do this.

## Part(a)

$x^{\text{prop}} = x^{\text{curr}}\times \text{exp}(y)$ will follow a $\text{LogNormal}(1, \text{exp}(\sigma^2))$. The proposal density is:
\[
\begin{aligned}
q(x^{\text{prop}} | x^{\text{curr}}) &= \frac{1}{C}(x^{\text{curr}}\times \text{exp}(y))^{-1}\text{exp}\bigg(\frac{-(\text{log}((x^{\text{curr}}\times \text{exp}(y))-1)^2}{2\text{exp}(\sigma^2)}\bigg) \nonumber \\
&= \frac{1}{C}(x^{\text{curr}}\times \text{exp}(y))^{-1}\text{exp}\bigg(\frac{-(\text{log}(x^{\text{curr}}) +  y-1)^2}{2\text{exp}(\sigma^2)}\bigg) \nonumber
\end{aligned}
\]


## Part(b)

The Rosenbluth-Hastings ratio corresponding to this proposal is:

\[
\begin{aligned}
\frac{f(x^\text{prop})}{f(x^\text{curr})}\frac{q(x^{\text{curr}} | x^{\text{prop}})}{q(x^{\text{prop}} | x^{\text{curr}})}
\end{aligned}
\]

where $f(x) = \frac{1}{Z}x^{3}\text{exp}(-\frac{x}{2})$ is the Gamma(4, 2) upto a normalizing constant $\frac{1}{Z}$ and $q(\cdot | \cdot)$ = $\frac{1}{C}(\text{exp}(y))^{-1}\text{exp}\bigg(-\frac{(\text{log}(\text{exp}(y) - 1)^{2}}{2\text{exp}(\sigma^{2})}\bigg)$ is the proposal density upto a normalizing constant $\frac{1}{C}$

Substituting $x^{\text{curr}}$ and $x^{\text{prop}} = x^{\text{curr}}\times \text{exp}(y)$ into this the Rosenbluth-Hastings ratio and simplifying I get:

\[
\boxed{x^{\text{curr}}\text{exp}(3y - \text{exp}(y))\text{exp}\bigg(\frac{-(y-1)^2 + (\text{log}(x^{\text{curr}})+y - 1)^2)}{2\text{exp}(\sigma^2)} \bigg)}
\]

# Part (c)

Here I add a function to perform this MCMC algorithm to my package, and document.


# Part (d)

Here is the approximation for $E(X^2)$ and the trace plot and a histogram of the samples.

```{r "MCMC estimate of second moment"}
set.seed(123)
source("/Users/dmwakima/Documents/09_R/07_Winter_23/STATS230/firstpkg/R/GammaMCMC.R")
iter <- 10000
init <- 1
tuning_param <- 2

mcmc_output <- matrix(NA, nrow=iter, ncol=2)
colnames(mcmc_output) <- c("state", "acceptance_status")

curr_state <- c(init, 1)
mcmc_output[1, ] <- curr_state

for (i in 2:iter){
  mcmc_output[i, ] <- lnorm_rw_next(mcmc_output[i-1, 1], tuning_param)
}

## acceptance probability
# mean(mcmc_output[, "acceptance_status"])

## second moment of X

y = mcmc_output[,"state"]

approx_second_moment <- cbind(mean(y^2), mean(mcmc_output[, "acceptance_status"]))

colnames(approx_second_moment) = c("Estimate", "Acceptance Probability")

approx_second_moment %>% 
  kable(format = "html",
        align = "c",
        digits = 4,
        caption = "Approximate Second Moment and Acceptance Probability",
        booktabs = TRUE) %>%
  kable_styling()
```


```{r "traceplots and histograms"}
par(mfrow=c(2, 2))

## trace plot
plot(1:iter,
     mcmc_output[, "state"],
     type="l", 
     xlab="Iteration", 
     ylab="MCMC State", 
     main="Trace Plot")

## histogram
hist(mcmc_output[,"state"], 
     xlab="MCMC State", 
     main="Target Distribution \n Histogram")
```






















































































































