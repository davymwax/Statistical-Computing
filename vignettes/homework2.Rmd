---
title: "Homework 2"
author: "David Mwakima"
date: "01/25/2023"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework2}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  rmarkdown.html_vignette.check_title = FALSE
)
```

## Question 1

Here I find by hand the Cholesky decomposition $L$ of the matrix $A$ below.

\[
A = \begin{pmatrix} 
2 & - 2 \\
-2 & 5
\end{pmatrix}
\]

I find it by solving the following matrix equation for $\ell_{11}$, $\vec{\ell}$ and $\ell_{22}$ by forward substitution.

\[
A = \begin{pmatrix} 
2 & - 2 \\
-2 & 5
\end{pmatrix} = \begin{pmatrix} \ell_{11} & \vec{\bf{0}} & \\
\vec{\ell} & \ell_{22}
\end{pmatrix} \begin{pmatrix} \ell_{11} & \ell_{11}^{T} & \\
\vec{\bf{0}}^{T} & \ell_{22}^{T}
\end{pmatrix}
\]

\[
\ell_{11}^{2} = 2  \\
\vec{\ell}\ell_{11} = -2 \\
\implies \ell_{11} = \boxed{\sqrt{2}}  \\
\implies \vec{\ell} = \boxed{\frac{-2}{\sqrt{2}}}
\]

\[
\ell_{11}\vec{\ell}^{T} = -2\\
\vec{\ell}\vec{\ell}^{T} + \ell_{22}\ell_{22}^{T}= 5 \\
\implies \text{From}\,\, \ell_{11} = \sqrt{2}\,\, \text{that}\,\, \vec{\ell}^{T} = \frac{-2}{\sqrt{2}} \\
\implies 2 + \ell_{22}^{2} = 5 \\
\implies \ell_{22} = \sqrt{5 - 2} = \boxed{\sqrt{3}}
\]

\[
\implies L = \begin{pmatrix}
\sqrt{2} & 0 \\
\frac{-2}{\sqrt{2}} & \sqrt{3}
\end{pmatrix}
\]

## Question 2

Supposing that the matrix $A = \{a_{ij} \}$ is banded in the sense that $a_{ij} = 0$ when $|i - j| < d$, I prove that the Cholesky decomposition $B = \{b_{ij} \}$ also satisfies the band condition.

$a_{ij} = 0$ when $|i - j| > d$ implies that the matrix $A = \{a_{ij}\}$ only has diagonal entries for some thresh-hold $d$. If $B = \{b_{ij} \}$ is the Cholesky decomposition of $A$, this means that $BB^{T} = A$. So we can write: 

\[
A = \begin{pmatrix} 
a_{11} & 0 \\
0 & \vec{a}_{22}
\end{pmatrix} = \begin{pmatrix} b_{11} & \vec{\bf{0}} & \\
\vec{b} & b_{22}
\end{pmatrix} \begin{pmatrix} b_{11} & \vec{b}^{T} & \\
\vec{\bf{0}}^{T} & b_{22}^{T}
\end{pmatrix} = \begin{pmatrix} 
b_{11}^{2} & b_{11}\vec{b}^{T} \\
\vec{b}b_{11} & \vec{b}b^{T} + b_{22}^{2}
\end{pmatrix}
\]

I need to argue that $\vec{b} = 0$. There are two cases to consider $b_{11}\vec{b}^{T}  = 0$ and $\vec{b}b_{11} = 0$. Since $b_{11}\vec{b}^{T}  = 0$, this means that $\vec{b}^{T} = 0$ since $a_{11} = b_{11}^{2} > 0$ implies that $b_{11} = \sqrt{a_{11}}$ is either less than $0$ or greater than $0$. Further since $bb_{11} = 0$, this means that $\vec{b} = 0$ since $b_{11} = \sqrt{a_{11}}$ is either less than $0$ or greater than $0$. In either case we find that $\vec{b} = 0$. So for $|i - j|>d$, $B = \{b_{ij}\}$ satisfies the band condition.

## Question 3

Here I show that if $X = QR$ is the $QR$ decomposition of $X$, where $X$ has linearly independent columns, that 
\[
X(X^TX)^{-1}X^T = QQ^T
\]

\[
\begin{aligned}
X(X^TX)^{-1}X^T &= (QR)((QR)^T(QR))^{-1}(QR)^T \\
&=(QR)(R^TQ^T(QR))^{-1}R^TQ^T \\
&=(QR)(R^TR)^{-1}R^TQ^T \,\,\,\text{since}\,\, Q^{T}Q = I \,\,\text{because}\,\, X \,\,\text{is full rank, $Q$ is orthonormal}\\
&=(QR)(R)^{-1}(R^T)^{-1}R^TQ^T\,\,\text{since} \,\,R\,\,\text{and}\,\,R^T \,\,\text{are invertible} \,\,\text{because}\,\, X \,\,\text{is full rank}\\
&=QQ^T \,\,\,\text{since}\,\,RR^{-1}= (R^{T})^{-1}R^{T} = I
\end{aligned}
\]

Further I show that $|\text{det}(X)| = |\text{det}(QR)|$ when $X$ is square.

\[
\begin{aligned}
|\text{det}(X)| &= |\text{det}(QR)| \\
 &= |\text{det}(Q)\text{det}(R)| \\
 &= |\pm1 \text{det}(R)| \,\,\,\text{since}\,\,Q \,\,\text{is orthonormal} \\
 &= |\text{det}(R)|
\end{aligned}
\]

In general I show that $\text{det}(X^TX) = [\text{det}(R)]^2$

\[
\begin{aligned}
\text{det}(X^TX) &= \text{det}((QR)^TQR)\\
          &= \text{det}(R^TQ^TQR) \\
          &= \text{det}(R^TR)\,\,\,\text{since}\,\,Q^TQ = I\\ 
          &= \text{det}(R^T)\text{det}(R) \\
          &= \text{det}(R)\text{det}(R) \,\,\,\text{since}\,\,R \,\,\text{is upper triangular diagonal of}\,\,R = \text{diagonal of}\,\, R^T\\ 
          &= [\text{det}(R)]^2
\end{aligned}
\]

## Question 4

Here I wrote an R function [`multvnorm()`](https://github.com/miningroup/your-stat-230-r-package-davymwax/blob/3ca9ddc9b6319547c0b8852d1f079e9bada7e357/R/multvnorm.R) that takes as input an $n$ dimensional numeric vector $\bf{\mu}$ and a $n \times n$ positive definite matrix $\bf{\Sigma}$ and returns $N$ realizations from a multivariate normal distribution $\text{MVN}(\bf{\mu}, \bf{\Sigma})$ using Cholesky decomposition.

I create a test case with $n = 4$ and $N = 100$ and use the sample mean and sample covariance matrices to validate my function. I also compare my function to the existing multivariate normal simulator [`mvrnorm()`](MASS::mvrnorm) implemented in the [MASS package](https://cran.r-project.org/web/packages/MASS/index.html).


```{r "Loading multvnorm and beta.estimator", echo=FALSE}
library(temp)
sim.data <- read.csv("/Users/dmwakima/Documents/09_R/07_Winter_23/STATS230/firstpkg/homework2_regression.csv")
```

```{r "Validating the simulation", echo=FALSE}
library(MASS)
set.seed(1247)
N = 100
n = 4
mu = sample(1:10, 4, replace=T)
X <- matrix(c(1.6, 1.2, 2.8, 3.6, 1.2, 5.8, 7.7, 4.1, 2.8, 7.7, 13.8, 13.4, 3.6, 4.1, 13.4, 43.1), byrow =T, nrow=4)
sigma = X

M_1 <- matrix(data = NA, nrow=4, ncol=100)
for (j in 1:100){
  M_1[, j] <- multvnorm(mu, sigma, N)[[j]]
  }

M_2 <- matrix(data = NA, nrow=4, ncol=100)
for (j in 1:100){
  M_2[, j] <- mvrnorm(N, mu, sigma)[j,]
}

my_function_mu <- apply(M_1,1,mean)
MASS_function_mu <- apply(M_2,1,mean)

compare_1 <- cbind(mu, my_function_mu, MASS_function_mu)
colnames(compare_1) <- c("Actual", "My Simulator", "MASS Package Simulator")

my_function_sigma <- apply(M_1, 1,var)
MASS_function_sigma <- apply(M_2,1, var)

compare_2 <- cbind(diag(sigma), my_function_sigma, MASS_function_sigma)
colnames(compare_2) <- c("Actual", "My Simulator", "MASS Package Simulator")
```


```{r "tab1", echo=FALSE}
library(kableExtra)
compare_1 %>%
  kable(format = "html",
        align = "c",
        digits = 4,
        caption = "Comparing Sample Mean Estimates",
        booktabs = TRUE)
```

```{r "tab2", echo=FALSE}
compare_2 %>%
  kable(format = "html",
        align = "c",
        digits = 4,
        caption = "Comparing Sample Variance",
        booktabs = TRUE)
```

Although, the sample mean results of `multvnorm()` are quite close to the actual sample mean, the sample variance results of `multvnorm()` are somewhat larger compared to the actual sample variance. However, the fact that larger values of actual sample variance correspond to larger values of the sample variance from `multvnorm()` suggests that at least something is right.

## Question 5

Here I obtain OLS estimates of the regression coefficients of a linear model fitted to the data in [`homework2_regression.csv`](https://github.com/miningroup/your-stat-230-r-package-davymwax/blob/3ca9ddc9b6319547c0b8852d1f079e9bada7e357/homework2_regression.csv). I implement two functions to obtain these estimates, [`chol.beta.est()`](https://github.com/miningroup/your-stat-230-r-package-davymwax/blob/3ca9ddc9b6319547c0b8852d1f079e9bada7e357/R/chol.beta.est.R) and [`QR.beta.est()`](https://github.com/miningroup/your-stat-230-r-package-davymwax/blob/3ca9ddc9b6319547c0b8852d1f079e9bada7e357/R/QR.beta.est.R). These functions use a Cholesky decomposition and QR decomposition, respectively. The implementation and documentation of the functions can be found [here](https://github.com/miningroup/your-stat-230-r-package-davymwax/tree/main/man). In paying attention to the order of operations so that my computations are performed efficiently, I used the function [`matvecprod()`](https://github.com/miningroup/your-stat-230-r-package-davymwax/blob/3ca9ddc9b6319547c0b8852d1f079e9bada7e357/R/matvecprod.R) I created in a previous assignment to calculate the product of matrices and vectors making use of the associative property $A(Bv) = (AB)v$ of matrix and vector multiplication.

```{r "Validating the function", include=FALSE}
head(sim.data)
sim.model <- lm(y~1+x2+x3+x4+x5, data = sim.data)
design_X <- as.matrix(sim.data[, c(2:6)])
y <- as.vector(sim.data[, 1])
c1 <- sim.model$coefficients
c2 <- chol.beta.est(design_X, y)
c3 <- QR.beta.est(design_X, y)
beta_estimates <- cbind(c1, c2, c3)
colnames(beta_estimates) <- c("lm function", "Cholesky", "QR")
```


```{r "tab3", echo=FALSE}
beta_estimates %>%
  kable(format = "html",
        align = "c",
        digits = 4,
        caption = "Comparing Regression Coefficient Estimates",
        booktabs = TRUE)
```

```{r "microbenchmarking", include=FALSE}
set.seed(1223)
library(bench)
chol_results <- as.data.frame(bench::mark(chol.beta.est(design_X, y)))
qr_results <- as.data.frame(bench::mark(QR.beta.est(design_X, y)))
results <- rbind(chol_results, qr_results)
rownames(results) <- c("Cholesky Decomposition", "QR Decomposition")
```

```{r "tab4", echo=FALSE}
results <- results[, c(2, 3)]
results %>%
  kable(format = "html",
        align = "c",
        digits = 4,
        caption = "Results of Microbenchmarking",
        booktabs = TRUE)
```

The results of benchmarking the computational efficiency of both implementations suggest that QR decomposition is about 2 to 3 times as fast as Cholesky decomposition. 


