---
title: "Homework 5"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework5}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE,
  fig.align = "center",
  fig.width = 5,
  fig.height = 5,
  rmarkdown.html_vignette.check_title = FALSE
)
```

```{r setup}
library(temp)
source("/Users/dmwakima/Documents/09_R/07_Winter_23/STATS230/firstpkg/R/EMfunction.R")
```

# Question 1

## Part (a)

Here I write the complete log-likelihood for the mixture model.

Let $\mathcal{l}(\alpha, \mu_{1}, \mu_{2})$ denote $\text{ln}\text{Pr}(\textbf{x}, \textbf{y}\,|\, \alpha, \mu_{1}, \mu_{2})$

See separate attachment.

## Part (b)

Here I obtain an algebraic form of \[E \bigg[ \mathcal{l}(\alpha, \mu_{1}, \mu_{2} \,|\, \textbf{y}, \alpha^{k}, \mu_{1}^{k}, \mu_{2}^{k}) \bigg]\], where the expectation is taken with respect to $\text{Pr}(\textbf{x}| \textbf{y}, \alpha^{(k)}, \mu_{1}^{(k)}, \mu_{2}^{(k)})$

See separate attachment.

## Part (c) 

Here I maximize the expectation from part (b) to obtain the iterative equations for the EM algorithm.

See separate attachment.


## Part (d)

Here I implement the EM algorithm and run it on the data saved in the file `mixture_data.txt`.

```{r "Test case"}
library(kableExtra)
test_data <- read.table("~/Documents/09_R/07_Winter_23/STATS230/firstpkg/data/mixture_data.txt", 
                        header = F)
y <- as.matrix(test_data)
theta <- c(0.5, 4, 8)
iter = 200
params <- as.matrix(EMfunction(y, theta, iter))
rownames(params) <- c("alpha", "mu1", "mu2")
params %>%
  kable(format = "html",
        align = "c",
        digits = 4,
        caption = "E-M Algorithm MLE estimates",
        booktabs = TRUE) %>%
  kable_styling()
```


## Part (e)

Here I conduct a simulation study to estimate the bias of the MLE for the mixture model above.

```{r "simulation study"}
set.seed(2)
##Getting simulated x_i
x <- matrix(data=NA, nrow=300, ncol=100)
for (i in 1:100){
    x[, i] <- rbinom(300, 1, 0.3)
}

##Getting simulated y_i
mu <- c(2.1, 5.2)
sd <- c(1, 0.8)
y <- matrix(data=NA, nrow=300, ncol=100)
for (i in 1:100){
  for (j in 1:300)
    y[, i] <- rnorm(300, mu[1+x[j, i]], sd[1+x[j, i]])
}

MLEs <- matrix(NA, nrow = 100, ncol=3)
for (i in 1:100){
  MLEs[i, ] <- EMfunction(y[, i], theta, iter=200)
}
```

Using these simulation results, I estimate the bias of the MLE for $\alpha$. 

```{r "bias of MLE_alpha"}
expected_alpha_hat <- mean(MLEs[, 1])
bias = expected_alpha_hat - 0.3
```

Finally, I estimate the associated Monte Carlo error.

```{r "montecarlo erro"}
sample_sd_alpha = sd(MLEs[, 1])
MC_error = 1.96*sample_sd_alpha
```

```{r "results"}
results <- rbind(bias, MC_error)
rownames(results) <- c("Bias", "Monte Carlo Error")
results %>%
  kable(format = "html",
        align = "c",
        digits = 4,
        caption = "Bias of MLE for alpha and Monte Carlo error",
        booktabs = TRUE) %>%
  kable_styling()
```

# Question 2

Here I show that if $(\bf{x}, \bf{y})$ form a hidden Markov model with 

\[
\text{Pr}(\textbf{x}, \textbf{y}) = \text{Pr}(x_{1}) \prod_{t = 2}^n \text{Pr}(x_{t}\,|\, x_{t - 1}) \prod_{t = 1}^n \text{Pr}(y_{t}\,|\,x_{t})
\]

then

\[
\text{Pr}(y_{i} \, | \, \textbf{x}_{1:i}, \textbf{y}_{1:i - 1}) = \text{Pr}(y_{i}\, |\, x_{i})\,\, \text{for} \,\, i = 1, \dots, n
\]

See attached document.


# Question 3

Here I consider the occasional dishonest casino hidden Markov model (HMM) with hidden states 1 = fair die, 2 = loaded die, transition probabilities 

\[
\textbf{P} = \begin{pmatrix}
0.98 & 0.02 \\
0.05 & 0.95
\end{pmatrix}
\]

emission probabilities 

\[
\textbf{E} = \begin{pmatrix}
\frac{1}{6} & \frac{1}{6} &  \frac{1}{6} &  \frac{1}{6} &  \frac{1}{6} &  \frac{1}{6} \\
\frac{1}{10} & \frac{1}{10} &  \frac{1}{2} &  \frac{1}{10} &  \frac{1}{10} &  \frac{1}{10} 
\end{pmatrix}
\]

and initial distribution 

\[
\nu^T = \bigg(\frac{1}{2}, \frac{1}{2}\bigg)
\]

## Part (a)

In this part I simulate $(\textbf{x}_{1:100}, \textbf{y}_{1:100})$ from this HMM and plot the hidden and observed states as time series.

```{r "simulating the HMM"}
set.seed(45)
n = 100
S = c(1, 2) #Hidden Markov variable state space
outcomes = c(1:6) #
D = c(0.5, 0.5) #initial distribution
P = cbind(c(0.98,0.05),c(0.02,0.95)) #transition probabilities
E = rbind(c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6),
          c(1/10, 1/10, 1/2, 1/10, 1/10, 1/10)) #emission probabilities

# Simulating the hidden Markov variable X
X = rep(0, n)
X[1] = sample(S, size=1, prob=D)
for(i in 1:(n-1)){
  X[i+1] = sample(S, size=1, prob=P[X[i],])
}

# Simulating the observed Y based on the emission probabilities.
Y = rep(0, n)
for (i in 1:n){
 Y[i] = sample(outcomes, size=1, prob=E[X[i], ])
}
```

```{r "plot1"}
time <- c(1:n)
plot(time, X, type = "l", col="red", ylab="Hidden States (1=Fair, 2=Loaded)",
     ylim = c(1, 2))
```

```{r "plot2"}
plot(time, Y, type="l", col="blue", ylab="observed states")
```

## Part (b)

Here I implement the forward and backward algorithms for the occasional dishonest casino example and run these algorithms on the observed states generated in part (a).

```{r "forward algorithm"}
A = matrix(NA, nrow=n, ncol=length(S))

#initialize A
A[1, ] <- D * E[, Y[1]]

#forward algorithm
for (t in 1:(n - 1)){
  A[t+1, ] = E[, Y[t + 1]]*(A[t, ] %*% P)
  A[t+1, ] = A[t+1, ]/sum(A[t+1, ])
}

# forward probabilities.
forward_prob <- sum(A[100, ])
```

```{r "backward algorithm"}
B = matrix(NA, nrow=n, ncol=length(S))

#initialize B
B[n, ] <- 1

#backward algorithm
for (t in (n-1):1){
  for(i in 1:length(S)){
B[t, i] = sum(B[t + 1, ]*P[i, ]*E[i:length(S), Y[t + 1]])
  }
B[t, ] = B[t, ]/sum(B[t, ])
}

#normalizing

# backward probabilities.
backward_prob <- (D[1] * E[1, Y[1]] * B[1, 1]) + (D[2] * E[2, Y[1]] * B[1, 2])
```

Finally here I compute marginal probabilities of hidden states at each time and plot them together with the true simulated values.

```{r "hidden marginal probabilities"}
H = A * B
hidden_prob <- H/rowSums(H)

plot(time, hidden_prob[, 1], type="l", col="red",
     ylab="Probability Fair Die", ylim=c(0, 1))
lines(time, X-1, type="l", col="black")
legend("left",
       legend = c("P(X_t = Fair)", "true state"),
       cex=0.75,
       col=c("red", "black"),
       lty=1)

plot(time, hidden_prob[, 2], type="l", col="blue",
     ylab="Probability Loaded Die", ylim=c(0, 1))
lines(time, X-1, type="l", col="black")
legend("topleft",
       legend = c("P(X_t = Loaded)", "true state"),
       cex=0.75,
       col=c("blue", "black"),
       lty=1)
```































