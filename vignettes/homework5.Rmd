---
title: "Homework 5"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework5}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE,
  rmarkdown.html_vignette.check_title = FALSE
)
```

```{r setup}
library(temp)
source("/Users/dmwakima/Documents/09_R/07_Winter_23/STATS230/firstpkg/R/EMfunction.R")
```

# Question 1

## Part (a)

Here I write the complete log-likelihood for the mixture model.

Let $\mathcal{l}(\alpha, \mu_{1}, \mu_{2})$ denote $\text{ln}\text{Pr}(\textbf{x}, \textbf{y}\,|\, \alpha, \mu_{1}, \mu_{2})$ 

## Part (b)

Here I obtain an algebraic form of \[E \bigg[ \mathcal{l}(\alpha, \mu_{1}, \mu_{2} \,|\, \textbf{y}, \alpha^{k}, \mu_{1}^{k}, \mu_{2}^{k}) \bigg]\], where the expectation is taken with respect to $\text{Pr}(\textbf{x}| \textbf{y}, \alpha^{(k)}, \mu_{1}^{(k)}, \mu_{2}^{(k)})$

## Part (c) 

Here I maximize the expectation from part (b) to obtain the iterative equations for the EM algorithm


## Part (d)

Here I implement the EM algorithm and run it on the data saved in the file `mixture_data.txt`.


## Part (e)

Here I conduct a simulation study to estimate the bias of the MLE for the mixture model above.

```{r "simulation study"}
set.seed(23263)
##Getting simulated x_i
x <- matrix(data=NA, nrow=300, ncol=100)
for (i in 1:100){
    x[, i] <- rbinom(300, 1, 0.3)
}

##Getting simulated y_i
mu <- c(2.1, 5.2)
sd <- c(1, 0.8)
y <- matrix(data=NA, nrow=300, ncol=100)
for (i in 1:100){
  for (j in 1:300)
    y[, i] <- rnorm(300, mu[1+x[j, i]], sd[1+x[j, i]])
}

MLEs <- matrix(NA, nrow = 100, ncol=3)
for (i in 1:100){
  MLEs[i, ] <- EMfunction(y[, i], theta, iter=200)
}
```

Using these simulation results, I estimate the bias of the MLE for $\alpha$. 

```{r "bias of MLE_alpha"}
expected_alpha_hat <- mean(MLEs[, 1])
bias = expected_alpha_hat - 0.3
bias
```

Finally, I estimate the associated Monte Carlo error.

```{r "montecarlo erro"}
sample_sd_alpha = sd(MLEs[, 1])
MC_error = 1.96*sample_sd_alpha
MC_error
```


# Question 2

Here I show that if $(\bf{x}, \bf{y})$ form a hidden Markov model with 

\[
\text{Pr}(\textbf{x}, \textbf{y}) = \text{Pr}(x_{1}) \prod_{t = 2}^n \text{Pr}(x_{t}\,|\, x_{t - 1}) \prod_{t = 1}^n \text{Pr}(y_{t}\,|\,x_{t})
\]

then

\[
\text{Pr}(y_{i} \, | \, \textbf{x}_{1:i}, \textbf{y}_{1:i - 1}) = \text{Pr}(y_{i}\, |\, x_{i})\,\, \text{for} \,\, i = 1, \dots, n
\]


# Question 3

Here I consider the occasional dishonest casino hidden Markov model (HMM) with hidden states 1 = fair die, 2 = loaded die, transition probabilities 

\[
\textbf{P} = \begin{pmatrix}
0.98 & 0.02 \\
0.05 & 0.95
\end{pmatrix}
\]

emission probabilities 

\[
\textbf{E} = \begin{pmatrix}
\frac{1}{6} & \frac{1}{6} &  \frac{1}{6} &  \frac{1}{6} &  \frac{1}{6} &  \frac{1}{6} \\
\frac{1}{10} & \frac{1}{10} &  \frac{1}{2} &  \frac{1}{10} &  \frac{1}{10} &  \frac{1}{10} 
\end{pmatrix}
\]

and initial distribution 

\[
\nu^T = \bigg(\frac{1}{2}, \frac{1}{2}\bigg)
\]

## Part (a)

In this part I simulate $(\textbf{x}_{1:100}, \textbf{y}_{1:100})$ from this HMM and plot the hidden and observed states as time series.


## Part (b)

Here I implement the forward and backward algorithms for the occasional dishonest casino example and run these algorithms on the observed states generated in part (a).

Here is a plot of the marginal probabilities of hidden states at each time together with the true simulated states.




















































